{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 데이터 작업하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch \r\n",
    "from torch import nn \r\n",
    "# 데이터셋을 반복 가능한 객체(iterable)화\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "# sample과 label 저장 \r\n",
    "from torchvision import datasets \r\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\"\"\" \r\n",
    "torchvision.datasets 모듈 \r\n",
    ": cifar, coco 등과 같은 다양한 실제 비전 데이터에 대한 Dataset을 포함\r\n",
    "\r\n",
    "Fasion MNIST 데이터셋을 사용해보자.\r\n",
    "\r\n",
    "이 튜토리얼에서는 Fasion MNIST 데이터셋을 사용한다.\r\n",
    "모든 TorchVision dataset은 샘플과 정답을 \r\n",
    "각각 변경하기 위한 transform과 target_transform의 두 인자를 포함한다.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "# # 공개 데이터셋에서 학습 데이터를 내려받습니다.\r\n",
    "training_data = datasets.FashionMNIST(\r\n",
    "    root=\"data\",\r\n",
    "    train=True,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor(),\r\n",
    ")\r\n",
    "\r\n",
    "# 공개 데이터셋에서 테스트 데이터를 내려받습니다.\r\n",
    "test_data = datasets.FashionMNIST(\r\n",
    "    root=\"data\",\r\n",
    "    train=False,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor(),\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\JIYEON\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"\"\"\r\n",
    "Dataset을 DataLoader의 인자로 전달한다.\r\n",
    "이는 데이터셋을 반복 가능한 객체(iterable)로 감싸고,\r\n",
    "자동화된 배치, 샘플링, shuffle 및 \r\n",
    "다중 프로세스로 데이터 불러오기(multiprocess data loading)를 지원한다.\r\n",
    "\r\n",
    "여기서 배치 크기는 64로 정의한다.\r\n",
    "즉, 데이터로더 객체의 각 요소는 64개의 특징과 정답을 묶음(batch)으로 반환한다.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "batch_size = 64\r\n",
    "\r\n",
    "# 데이터 로더 생성\r\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\r\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\r\n",
    "\r\n",
    "for X, y in test_dataloader:\r\n",
    "    print(\"shape of X [N, C, H, W] :\", X.shape)\r\n",
    "    print(\"Shape of y :\", y.shape, y.dtype)\r\n",
    "    break\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of X [N, C, H, W] : torch.Size([64, 1, 28, 28])\n",
      "Shape of y : torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "\"\"\"\r\n",
    "파이토치에서 신경망 모델은 nn.Module을 상속받는 클래스를 생성하여 정의\r\n",
    "__init__함수에서 신경망의 계층을 정의하고\r\n",
    "forward 함수에서 신경망에 데이터를 어떻게 전달할지 정한다.\r\n",
    "\r\n",
    "가능한 경우 gpu로 신경망을 이동시켜 연산을 가속(accelerate)한다.\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "# 학습에 사용할 cpu/gpu 장치 등록\r\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "print(\"Using {} device\".format(device))\r\n",
    "\r\n",
    "# 모델 정의\r\n",
    "class NeuralNetwork(nn.Module):\r\n",
    "    # 신경망 계층 초기화 \r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "        self.flatten = nn.Flatten()\r\n",
    "        self.linear_relu_stack = nn.Sequential(\r\n",
    "            nn.Linear(28*28, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512, 10),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.flatten(x)\r\n",
    "        logits = self.linear_relu_stack(x)\r\n",
    "        return logits\r\n",
    "\r\n",
    "\r\n",
    "# NeuralNetwork의 인스턴스를 생성하고 \r\n",
    "# 이를 device로 이동한 뒤, \r\n",
    "# 구조를 출력\r\n",
    "model = NeuralNetwork().to(device)\r\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 신경망 모델 구성하기 - 자세히"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "\"\"\"\r\n",
    "모델을 사용하기 위해 입력데이터 전달\r\n",
    "이는 일부 백그라운드 연산들과 함께 모들의 forward를 실행\r\n",
    "model.forward() 직접 호출하지 말기\r\n",
    "--> 모델에 입력을 호출하면\r\n",
    "각 클래스에 대한 raw 예측값이 있는 10차원 텐서가 반환됨\r\n",
    "raw 예측값을 nn.Sotfmax 모듈의 인스턴스에 통과시켜\r\n",
    "예측 확률을 얻는다.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "X = torch.rand(1, 28, 28, device=device)\r\n",
    "logits = model(X)\r\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\r\n",
    "y_pred = pred_probab.argmax(1)\r\n",
    "print(f\"Predicted class: {y_pred}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted class: tensor([256], device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# print(pred_probab)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 모델 계층"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "\"\"\"\r\n",
    "Fashion MNIST 모델의 계층들을 살펴보자.\r\n",
    "이를 설명하기 위해 28x28 크기의 이미지 3개로 구성된 미니배치를 가져와,\r\n",
    "신경망을 통과할 때 어떤 일이 발생하는지 보자\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "input_image = torch.rand(3, 28, 28)\r\n",
    "print(input_image.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "\"\"\" nn Flatten\r\n",
    "nn.Flatten 계층을 초기화하여 \r\n",
    "각 28x28의 2D 이미지를 784 픽셀 값을 갖는 연속된 배열로 변환\r\n",
    "dim=0의 미니배치 차원은 유지된다.\r\n",
    "\"\"\"\r\n",
    "flatten = nn.Flatten()\r\n",
    "flat_image = flatten(input_image)\r\n",
    "print(flat_image.size())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "\"\"\" nn.Linear\r\n",
    "선형 계층은 저장된 가중치와 편향을 사용하여 \r\n",
    "입력에 선형 변환(linear transformation)을 적용하는 모듈\"\"\"\r\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=28)\r\n",
    "hidden1 = layer1(flat_image)\r\n",
    "print(hidden1.size())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 28])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "\"\"\"nn.ReLU\r\n",
    "비선형 activation은 모델의 입력과 출력 사이에 복잡한 관계(mapping)을 만든다.\r\n",
    "비선형 활성화는 선형 변환 후에 적용되어 비선형성(nonlinearity)을 도입하고,\r\n",
    "신경망이 다양한 현상을 학습할 수 있도록 돕는다.\"\"\"\r\n",
    "print(f\"Before ReLu: {hidden1}\\n\\n\")\r\n",
    "hidden1 = nn.ReLU()(hidden1)\r\n",
    "print(f\"After Relu: {hidden1}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before ReLu: tensor([[-0.6363, -0.2606, -0.0175,  0.3945,  0.5081, -0.1264,  0.0226, -0.4330,\n",
      "         -0.3636,  0.4240, -0.0865, -0.4544, -0.3437, -0.0628,  0.1276, -0.0256,\n",
      "          0.2602, -0.0348,  0.1153,  0.0191, -0.4050, -0.4278, -0.1196, -0.0766,\n",
      "          0.3642, -0.1131,  0.1299, -0.2919],\n",
      "        [-0.1992, -0.3360,  0.1045,  0.4432,  0.4421,  0.1374,  0.1344, -0.4782,\n",
      "         -0.0836,  0.2932, -0.0063, -0.3421, -0.1063,  0.2151,  0.0242,  0.0509,\n",
      "         -0.3092, -0.0977, -0.0698,  0.0470, -0.1319, -0.6934,  0.1303, -0.0984,\n",
      "          0.1728,  0.3856,  0.0517, -0.3452],\n",
      "        [-0.3318, -0.1422, -0.0357,  0.0116,  0.7501, -0.0359,  0.0706, -0.2123,\n",
      "         -0.2579,  0.3249,  0.2102, -0.0418, -0.3997,  0.3422, -0.0468, -0.2178,\n",
      "          0.0534,  0.1922, -0.4640,  0.0874, -0.1810, -0.3468, -0.1928,  0.0176,\n",
      "          0.3929,  0.2510,  0.0867, -0.2786]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After Relu: tensor([[0.0000, 0.0000, 0.0000, 0.3945, 0.5081, 0.0000, 0.0226, 0.0000, 0.0000,\n",
      "         0.4240, 0.0000, 0.0000, 0.0000, 0.0000, 0.1276, 0.0000, 0.2602, 0.0000,\n",
      "         0.1153, 0.0191, 0.0000, 0.0000, 0.0000, 0.0000, 0.3642, 0.0000, 0.1299,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1045, 0.4432, 0.4421, 0.1374, 0.1344, 0.0000, 0.0000,\n",
      "         0.2932, 0.0000, 0.0000, 0.0000, 0.2151, 0.0242, 0.0509, 0.0000, 0.0000,\n",
      "         0.0000, 0.0470, 0.0000, 0.0000, 0.1303, 0.0000, 0.1728, 0.3856, 0.0517,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0116, 0.7501, 0.0000, 0.0706, 0.0000, 0.0000,\n",
      "         0.3249, 0.2102, 0.0000, 0.0000, 0.3422, 0.0000, 0.0000, 0.0534, 0.1922,\n",
      "         0.0000, 0.0874, 0.0000, 0.0000, 0.0000, 0.0176, 0.3929, 0.2510, 0.0867,\n",
      "         0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "\"\"\"nn.Sequential \r\n",
    ": 순서를 갖는 모듈의 컨테이너\r\n",
    "데이터는 정의된 것과 같은 순서로 모든 모듈을 통해 전달된다.\r\n",
    "순차 컨테이너를 사용하여 아래의 seq_modules와 같은 신경망을 빠르게 만들 수 있다.\r\n",
    " \"\"\"\r\n",
    "\r\n",
    "seq_modules = nn.Sequential(\r\n",
    "    flatten,\r\n",
    "    layer1,\r\n",
    "    nn.ReLU(),\r\n",
    "    nn.Linear(28,10)\r\n",
    ")\r\n",
    "\r\n",
    "input_image = torch.rand(3, 28, 28)\r\n",
    "logits = seq_modules(input_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 모델 매개변수"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "\"\"\"\r\n",
    "신경망 내부의 많은 계층들은 매개변수화(parameterize) 된다.\r\n",
    "즉, 학습 중에 최적화되는 가중치와 편향과 연관지어진다.\r\n",
    "nn.Module을 상속하여 모델 객체 내부의 모든 필드들이 자동으로 추적(track)되며,\r\n",
    "모델의 parameters() 및 named_parameters() 메소드로 \r\n",
    "모든 매개변수에 접근할 수 있게 된다. \"\"\"\r\n",
    "print(\"Model structure: \", model, \"\\n\\n\")\r\n",
    "\r\n",
    "for name, param in model.named_parameters():\r\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values :{param[:2]} \\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values :tensor([[ 0.0152,  0.0234,  0.0158,  ...,  0.0062,  0.0210,  0.0283],\n",
      "        [ 0.0236,  0.0042, -0.0035,  ..., -0.0182, -0.0167,  0.0088]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values :tensor([-0.0090,  0.0249], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values :tensor([[ 0.0183,  0.0141,  0.0418,  ..., -0.0267, -0.0028, -0.0264],\n",
      "        [-0.0421,  0.0430,  0.0350,  ..., -0.0058, -0.0168,  0.0283]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values :tensor([ 0.0220, -0.0244], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values :tensor([[ 0.0225,  0.0274, -0.0373,  ..., -0.0067, -0.0063, -0.0210],\n",
      "        [ 0.0255,  0.0306,  0.0302,  ..., -0.0306, -0.0247, -0.0089]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values :tensor([-0.0404, -0.0226], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 매개변수 최적화하기 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# 모델을 학습하려면 손실함수와 옵티마이저가 필요하다.\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# 각 학습 단계(training loop)에서 모델은 \r\n",
    "# (배치로 제공되는) 학습 데이터셋에 대한 예측을 수행하고, \r\n",
    "# 예측 오류를 역전파하여 모델의 매개변수를 조정한다.\r\n",
    "\r\n",
    "def train(dataloader, model, loss_fn, optimizer):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    for batch, (X,y) in enumerate(dataloader):\r\n",
    "        X, y = X.to(device), y.to(device)\r\n",
    "\r\n",
    "        # 예측 오류 계산\r\n",
    "        pred = model(X)\r\n",
    "        loss = loss_fn(pred, y)\r\n",
    "\r\n",
    "        # 역전파 \r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        if batch % 100 == 0:\r\n",
    "            loss, current = loss.item(), batch*len(X)\r\n",
    "            print(f\"loss: {loss:>7f}    [{current:>5d}/{size:>5d}]\")    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "\"\"\"\r\n",
    "모델이 학습하고 있는지 확인하기 위해\r\n",
    "테스트 데이터셋으로 모델의 성능을 확인한다.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "def test(dataloader, model, loss_fn):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    num_batches = len(dataloader)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    test_loss, correct = 0, 0\r\n",
    "    with torch.no_grad():\r\n",
    "        for X, y in dataloader:\r\n",
    "            X, y = X.to(device), y.to(device)\r\n",
    "            pred = model(X)\r\n",
    "            test_loss += loss_fn(pred, y).item()\r\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\r\n",
    "        \r\n",
    "        test_loss /= num_batches\r\n",
    "        correct /= size\r\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\r\n",
    "\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "\"\"\"\r\n",
    "학습 단계는 여러번의 반복(에폭) 단계를 거쳐 수행된다.\r\n",
    "각 에폭에서는 모델은 더 나은 예측을 하기 위해 매개변수를 학습한다.\r\n",
    "각 에폭마다 모델의 정확도와 손실을 출력한다.\r\n",
    "에폭마다 정확도가 증가하고 손실이 감소하는 것을 보려고 한다.\r\n",
    "\"\"\"\r\n",
    "epochs = 5\r\n",
    "for t in range(epochs):\r\n",
    "    print(f\"Epoch {t+1}\\n-----------------------\")\r\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\r\n",
    "    test(test_dataloader, model, loss_fn)\r\n",
    "print(\"Done!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-----------------------\n",
      "loss: 1.784283    [    0/60000]\n",
      "loss: 1.699161    [ 6400/60000]\n",
      "loss: 1.797474    [12800/60000]\n",
      "loss: 1.764295    [19200/60000]\n",
      "loss: 1.634640    [25600/60000]\n",
      "loss: 1.918155    [32000/60000]\n",
      "loss: 1.660005    [38400/60000]\n",
      "loss: 1.895247    [44800/60000]\n",
      "loss: 1.763144    [51200/60000]\n",
      "loss: 1.704725    [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 1.719640 \n",
      "\n",
      "Epoch 2\n",
      "-----------------------\n",
      "loss: 1.757629    [    0/60000]\n",
      "loss: 1.671656    [ 6400/60000]\n",
      "loss: 1.771859    [12800/60000]\n",
      "loss: 1.746646    [19200/60000]\n",
      "loss: 1.612813    [25600/60000]\n",
      "loss: 1.898856    [32000/60000]\n",
      "loss: 1.640238    [38400/60000]\n",
      "loss: 1.881477    [44800/60000]\n",
      "loss: 1.742133    [51200/60000]\n",
      "loss: 1.690654    [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 1.700365 \n",
      "\n",
      "Epoch 3\n",
      "-----------------------\n",
      "loss: 1.733078    [    0/60000]\n",
      "loss: 1.646955    [ 6400/60000]\n",
      "loss: 1.748887    [12800/60000]\n",
      "loss: 1.732134    [19200/60000]\n",
      "loss: 1.595047    [25600/60000]\n",
      "loss: 1.881858    [32000/60000]\n",
      "loss: 1.622614    [38400/60000]\n",
      "loss: 1.869255    [44800/60000]\n",
      "loss: 1.723284    [51200/60000]\n",
      "loss: 1.677609    [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.683170 \n",
      "\n",
      "Epoch 4\n",
      "-----------------------\n",
      "loss: 1.709461    [    0/60000]\n",
      "loss: 1.623572    [ 6400/60000]\n",
      "loss: 1.727555    [12800/60000]\n",
      "loss: 1.720003    [19200/60000]\n",
      "loss: 1.579808    [25600/60000]\n",
      "loss: 1.867093    [32000/60000]\n",
      "loss: 1.605877    [38400/60000]\n",
      "loss: 1.858045    [44800/60000]\n",
      "loss: 1.705310    [51200/60000]\n",
      "loss: 1.665839    [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.667308 \n",
      "\n",
      "Epoch 5\n",
      "-----------------------\n",
      "loss: 1.685988    [    0/60000]\n",
      "loss: 1.600963    [ 6400/60000]\n",
      "loss: 1.707466    [12800/60000]\n",
      "loss: 1.709417    [19200/60000]\n",
      "loss: 1.566364    [25600/60000]\n",
      "loss: 1.853788    [32000/60000]\n",
      "loss: 1.589861    [38400/60000]\n",
      "loss: 1.846388    [44800/60000]\n",
      "loss: 1.688027    [51200/60000]\n",
      "loss: 1.654955    [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.652363 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "% nvidia-smi\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 저장하기 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "\"\"\"\r\n",
    "모델을 저장하는 일반적인 방법은 (모델의 매개변수들 포함)\r\n",
    "내부 상태 사전(internal state dictionary)을 직렬화(serialize)하는 것\r\n",
    "\"\"\"\r\n",
    "torch.save(model.state_dict(), \"./test.pth\")\r\n",
    "print(\"Saved PyTorch Model State to model.pth\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 불러오기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "\"\"\"\r\n",
    "모델을 불러오는 과정에는\r\n",
    "1. 모델 구조를 다시 만들고\r\n",
    "2. 상태 사전을 모델에 불러오는 과정\r\n",
    "이 포함된다.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "model = NeuralNetwork()\r\n",
    "model.load_state_dict(torch.load(\"test.pth\"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "\r\n",
    "\"\"\"\r\n",
    "이제 모델을 사용하여 예측할 수 있다.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "classes = [\r\n",
    "    \"T-shirt/top\",\r\n",
    "    \"Trouser\",\r\n",
    "    \"Pullover\",\r\n",
    "    \"Dress\",\r\n",
    "    \"Coat\",\r\n",
    "    \"Sandal\",\r\n",
    "    \"Shirt\",\r\n",
    "    \"Sneaker\",\r\n",
    "    \"Bag\",\r\n",
    "    \"Ankle boot\"\r\n",
    "]\r\n",
    "\r\n",
    "model.eval()\r\n",
    "x, y = test_data[0][0], test_data[0][1]\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    pred = model(x)\r\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\r\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted: \"Sandal\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "cd7d8ae84c30c9b148779bedbfd4b6b894c49078e9fff681c5971088fae95644"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}